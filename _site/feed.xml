<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-10-10T22:07:16+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ganesh Bannur</title><subtitle>Ganesh Bannur</subtitle><entry><title type="html">Survey of text detection and recognition</title><link href="http://localhost:4000/2023/05/28/text-survey.html" rel="alternate" type="text/html" title="Survey of text detection and recognition" /><published>2023-05-28T00:00:00+05:30</published><updated>2023-05-28T00:00:00+05:30</updated><id>http://localhost:4000/2023/05/28/text-survey</id><content type="html" xml:base="http://localhost:4000/2023/05/28/text-survey.html">&lt;p&gt;This post is a brief survey of the techniques in text detection and recognition.&lt;/p&gt;

&lt;p&gt;Extracting text from images, called text spotting, is arguably the most important technique for extracting information from images. This is because text is the most dense representation of information present in images. Real-world scenes are rich in text and identifying the text present in a scene can vastly improve the understanding of the scene.&lt;/p&gt;

&lt;p&gt;Traditionally text spotting has been divided into text detection and text recognition, both performed by separate methods. This is true even of recent deep learning methods. There are dedicated text detection and recognition models. Recently there has been an increased effort to train end-to-end text spotting models, which are single models that recognize the text in an image in one shot. These methods are expected to have better results due to information sharing between the tasks of detection and recognition. While these end-to-end methods are improving they are still behind pipelined methods which break the task down into detection and recognition. This is evidenced by [&lt;strong&gt;1&lt;/strong&gt;], in which they state&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It’s widely believed that end-to-end models enjoy shared feature extraction which leads to better accuracy. However, the results of our competition say otherwise.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In that spirit the rest of this post gives surveys and comparisons of text detection and recognition techniques. However, keep in mind that only papers with official code implementations were considered since I was surveying techniques to integrate them into a pipeline.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;detection&quot;&gt;Detection&lt;/h1&gt;
&lt;p&gt;Here is a comparison of text detection techniques. For each technique the result tables from the corresponding paper are given. The first table condenses the results of all the techniques. There is also a reference image given below the table which is used to demonstrate the output of the techniques. I ultimately chose to use the &lt;em&gt;unified detector&lt;/em&gt; from [&lt;strong&gt;2&lt;/strong&gt;] since it produced the best results empirically. However the &lt;em&gt;boundary transformer&lt;/em&gt; from [&lt;strong&gt;3&lt;/strong&gt;] is the best on paper and may be better in other scenarios.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/text-survey/Text-Detection.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/text-survey/Text-Detection.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;recognition&quot;&gt;Recognition&lt;/h1&gt;
&lt;p&gt;Here is a survey of text recognition techniques. The result tables for some interesting techniques are given.&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/text-survey/text-recognition.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/text-survey/text-recognition.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Long, Shangbang, et al. “ICDAR 2023 Competition on Hierarchical Text Detection and Recognition.” arXiv preprint arXiv:2305.09750 (2023).
[2] Long, Shangbang, et al. “Towards end-to-end unified scene text detection and layout analysis.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
[3] Zhang, Shi-Xue, et al. “Arbitrary shape text detection via boundary transformer.” IEEE Transactions on Multimedia (2023).&lt;/p&gt;</content><author><name></name></author><summary type="html">This post is a brief survey of the techniques in text detection and recognition.</summary></entry><entry><title type="html">Survey of vision and vision-language tasks for robots</title><link href="http://localhost:4000/2023/01/10/v-and-vl-survey.html" rel="alternate" type="text/html" title="Survey of vision and vision-language tasks for robots" /><published>2023-01-10T00:00:00+05:30</published><updated>2023-01-10T00:00:00+05:30</updated><id>http://localhost:4000/2023/01/10/v-and-vl-survey</id><content type="html" xml:base="http://localhost:4000/2023/01/10/v-and-vl-survey.html">&lt;p&gt;This is a survey of techniques and datasets for some vision and vision-language tasks. The survey was done on tasks which could be useful to a robot. A variety of tasks are explored:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Extraction of relevant information from the surrounding scene&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  This is done through the robot’s vision pipeline. Different extraction methods are applied on the robot’s video stream to extract different types of information from the scene. Object detection, event detection, human action detection and text detection and recognition are some of the key extraction methods applied. Text detection and recognition techniques are explored in more detail &lt;a href=&quot;/2023/01/10/v-and-vl-survey.html&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
  Retrieving extracted information based on natural language inputs provided by humans is also very important for a robot. Some techniques to retrieve objects in the scene based on their description are also explored.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Navigation&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  Navigation is a fundamental task for a robot. Recent techniques have explored navigation as a vision-language task in which the robot must navigate based on natural language commands and its video stream.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Gaining visual knowledge&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  In order to perform generic tasks such as zero-shot navigation in a new environment or visual-question answering based on the current scene, a robot must have large scale visual knowledge. A robot can either query an external knowledge base or accumulate knowledge in the parameters of a deep learning model. Recent work has explored the latter since end-to-end models with large scale visual knowledge perform better at visual tasks. Some papers proposing frameworks for large scale visual knowledge acquisition are explored.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Miscellaneous&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  Other interesting papers on datasets, custom architectures for video tasks, pretraining and finding specific moments in video are also surveyed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Papers proposing techniques for these categories are listed. The key ideas put forth by papers in each category are summarized.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/v-vl-survey/Vision-Query-Literature-Survey.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/v-vl-survey/Vision-Query-Literature-Survey.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Here is a survey of datasets available for the tasks mentioned above.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/v-vl-survey/Vision-Query-Datasets.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/v-vl-survey/Vision-Query-Datasets.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;predicting-the-future&quot;&gt;Predicting the future&lt;/h1&gt;
&lt;p&gt;Humans are able to predict the future. Well, at least in the short term. We are able to judge facial expressions and predict what a person may do, observe movements and predict where a person is headed and in general predict future actions based on familiar visual cues received till now. Predicting the intent or next action of a human based on current and past video frames is crucial for robots to be proactive rather than reactive when interacting with humans. Predicting the short term (or even better - long term) goals of a person will let a robot take preemptive action. Current work focuses on video-based prediction as it is easy to acquire a video feed from a robot. [&lt;strong&gt;1&lt;/strong&gt;] justifies this by showing that video-based prediction is not inferior to using 3D kinematics, which is able to capture every dimension of human motion.&lt;/p&gt;

&lt;p&gt;Here is a survey of techniques that have been used for predicting human actions and intents&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/v-vl-survey/Vision-Based-Intent.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/v-vl-survey/Vision-Based-Intent.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;And here are some datasets that exist for this task&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/v-vl-survey/Vision-Based-Intent-Datasets.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/v-vl-survey/Vision-Based-Intent-Datasets.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Zunino, Andrea, et al. “What will i do next? The intention from motion experiment.” Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017.&lt;/p&gt;</content><author><name></name></author><summary type="html">This is a survey of techniques and datasets for some vision and vision-language tasks. The survey was done on tasks which could be useful to a robot. A variety of tasks are explored: Extraction of relevant information from the surrounding scene This is done through the robot’s vision pipeline. Different extraction methods are applied on the robot’s video stream to extract different types of information from the scene. Object detection, event detection, human action detection and text detection and recognition are some of the key extraction methods applied. Text detection and recognition techniques are explored in more detail here. Retrieving extracted information based on natural language inputs provided by humans is also very important for a robot. Some techniques to retrieve objects in the scene based on their description are also explored. Navigation Navigation is a fundamental task for a robot. Recent techniques have explored navigation as a vision-language task in which the robot must navigate based on natural language commands and its video stream. Gaining visual knowledge In order to perform generic tasks such as zero-shot navigation in a new environment or visual-question answering based on the current scene, a robot must have large scale visual knowledge. A robot can either query an external knowledge base or accumulate knowledge in the parameters of a deep learning model. Recent work has explored the latter since end-to-end models with large scale visual knowledge perform better at visual tasks. Some papers proposing frameworks for large scale visual knowledge acquisition are explored. Miscellaneous Other interesting papers on datasets, custom architectures for video tasks, pretraining and finding specific moments in video are also surveyed.</summary></entry><entry><title type="html">Conversational Agents: A survey of techniques, frameworks and datasets</title><link href="http://localhost:4000/2022/11/01/ca-survey.html" rel="alternate" type="text/html" title="Conversational Agents: A survey of techniques, frameworks and datasets" /><published>2022-11-01T00:00:00+05:30</published><updated>2022-11-01T00:00:00+05:30</updated><id>http://localhost:4000/2022/11/01/ca-survey</id><content type="html" xml:base="http://localhost:4000/2022/11/01/ca-survey.html">&lt;p&gt;This post is a survey of conversational agents that I did when I was working to build one myself.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;architecture&quot;&gt;Architecture&lt;/h1&gt;
&lt;p&gt;The general architecture of a conversational agent is given in &lt;em&gt;Figure 1&lt;/em&gt;. Traditional techniques follow this architectural division closely and implement specialized modules for each component of the agent. However it is becoming increasingly common to combine one or more components in favour of more end-to-end conversational agents. This is especially true when using large deep learning models as they are able to implement more than one component, often with better results due to information sharing between the tasks of individual components.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ca/architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h4 align=&quot;center&quot;&gt;
&lt;i&gt;Figure 1: Architecture of a Conversational Agent&lt;/i&gt;
&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;state-of-the-art-techniques&quot;&gt;State-of-the-art techniques&lt;/h1&gt;
&lt;p&gt;Here’s a survey of state-of-the-art techniques for building entire conversational agents or one or more individual modules shown in &lt;em&gt;Figure 1&lt;/em&gt;. All papers are linked in the table and also at the end. A few other interesting papers, papers on human-in-the-loop techniques and studies on building conversational agents are also linked at the end.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Literature-Search.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Literature-Search.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4&gt;State-of-the-art techniques for each module&lt;/h4&gt;
&lt;p&gt;Here is a grouping of papers based on which architectural component their techniques apply to. The key points of each paper are summarized.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Papers-By-CA-Module.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Papers-By-CA-Module.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;frameworks&quot;&gt;Frameworks&lt;/h1&gt;
&lt;p&gt;Frameworks are an important starting point for the development of a conversational agent as they can provide a standard data format, pretrained models, dialog system pipelines, etc. I ultimately chose Rasa to implement my conversational agent but there are many frameworks out there suited for a wide variety of tasks. Here’s a survey of the most popular platforms and frameworks available.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Platforms-And-Tools.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Platforms-And-Tools.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;datasets&quot;&gt;Datasets&lt;/h1&gt;
&lt;p&gt;Datasets are necessary for training custom models to use for language understanding/dialog management or for training end-to-end conversational agents. Good quality datasets are very important for dialog applications since the agent needs to produce good quality text to hold a satisfying conversation. Here is a survey of the most popular datasets available for training conversational agents.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Datasets.pdf&quot;&gt;&lt;center&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/center&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Datasets.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;additional-notes-on-select-papers&quot;&gt;Additional notes on select papers&lt;/h1&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Paper-Notes.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Paper-Notes.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;&lt;i&gt;Speech and Language Processing&lt;/i&gt; by &lt;i&gt;Dan Jurafsky&lt;/i&gt; and &lt;i&gt;James H. Martin&lt;/i&gt;&lt;br /&gt;
Chapter 15: &lt;i&gt;Chatbots &amp;amp; Dialogue Systems&lt;/i&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;https://web.stanford.edu/~jurafsky/slp3/15.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Conversational Agents: Theory and Applications&lt;/i&gt; by &lt;i&gt;Mattias Wahde&lt;/i&gt; and &lt;i&gt;Marco Virgolin&lt;/i&gt;
&lt;a href=&quot;&quot;&gt;https://browse.arxiv.org/pdf/2202.03164.pdf&lt;/a&gt;&lt;br /&gt;
Here’s an extremely short summary of the key points they present.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Conversational-Agents-Theory-and-Applications-Summary.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Conversational-Agents-Theory-and-Applications-Summary.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">This post is a survey of conversational agents that I did when I was working to build one myself. Architecture The general architecture of a conversational agent is given in Figure 1. Traditional techniques follow this architectural division closely and implement specialized modules for each component of the agent. However it is becoming increasingly common to combine one or more components in favour of more end-to-end conversational agents. This is especially true when using large deep learning models as they are able to implement more than one component, often with better results due to information sharing between the tasks of individual components.</summary></entry><entry><title type="html">MODALS: Data augmentation that works for everyone</title><link href="http://localhost:4000/2022/01/14/modals.html" rel="alternate" type="text/html" title="MODALS: Data augmentation that works for everyone" /><published>2022-01-14T00:00:00+05:30</published><updated>2022-01-14T00:00:00+05:30</updated><id>http://localhost:4000/2022/01/14/modals</id><content type="html" xml:base="http://localhost:4000/2022/01/14/modals.html">&lt;p&gt;&lt;br /&gt;
In this post we discuss the paper “MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space”[1]
&lt;br /&gt;
&lt;br /&gt;
Data augmentation is important for all types of deep learning tasks: unsupervised, supervised and self-supervised. Data augmentation has almost become a prerequisite for deep learning. In fact many self-supervised learning tasks [2] use data augmentation as a core component of the learning algorithm. It is also heavily used in image models and has consistently produced good results. By perturbing the data during training we are able to train more robust models, especially when there is limited data. Data augmentation is also very useful in incorporating known priors or domain knowledge into the model. For example the random flip transform applied to image data implicitly encodes the prior knowledge we have that the data is invariant to changes in orientation.
&lt;br /&gt;
&lt;br /&gt;
The usefulness of data augmentation has led to the development of specific techniques of augmentation unique to each modality of data. The techniques developed for one modality usually suit the type of data in that particular modality. For image data some commonly used augmentation techniques are rotation, cropping, applying affine transforms, random flips, contrast and color augmentations and adding Gaussian blur. More recent techniques like CutMix[3] and MixUp[4] apply data augmentation on both the image and label space. Many robust data augmentation techniques for image data already exist and are used widely. However modalities like tables and graph data don’t have as many robust augmentation techniques. Since the techniques developed for images were made taking into consideration the nature of image data, they usually cannot be applied to other modalities. If a generalized, modality-agnostic framework for augmentation could be developed then standard, robust augmentation techniques can be applied across many modalities. This is exactly what the authors of the paper propose.
&lt;br /&gt;
&lt;br /&gt;
In order to be independent of modality, such a framework would have to convert data from different modalities to a common representation on which label-preserving transforms can be applied. The authors point out that there exist techniques of data augmentation that use autoencoders to learn a latent space representation [5]. This is then used to generate augmentation data for the downstream task. In the framework proposed by the paper the learning of the latent space is integrated with the downstream task. A model is trained for the downstream task and an intermediate layer encodes the representations of the input data in the latent space. The framework uses interpolation and extrapolation of existing data points to produce augmented data. This requires that the latent space learned must be smooth (the representations cannot vary abruptly for a minute change in input features) for transformations to be label preserving and produce valid data. The smoothness ensures that the augmented data is inside the data manifold (ensuring it is valid data) and has the same label as the points used to generate it. To ensure this property the authors add two additional losses to the encoder used to learn the latent space. These are the adversarial loss and the triplet loss.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Adversarial Loss&lt;/h3&gt;
&lt;p&gt;To achieve a smooth latent space data manifold we use a discriminator network and add an adversarial loss to the model. The discriminator, given a data point, tries to predict if it is from the latent space or is a randomly sampled point from a Gaussian distribution. To keep the adversarial loss low (to “fool” the discriminator network) the model must also learn a smooth data representation so as to be indistinguishable from a smooth Gaussian distribution.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Triplet Loss&lt;/h3&gt;
&lt;p&gt;The aim is to have the representations of all the data points of a class to be close to each other and far away from data points from other classes. To learn the latent space in this way we add the triplet loss to the model. The Triplet loss pulls together samples from the same class and pushes away samples from other classes.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;br /&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/PaperFig2.png&quot; alt=&quot;Figure 2 from the paper&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: The MODALS framework (figure from paper)
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Transformations in the latent space&lt;/h2&gt;
&lt;p&gt;Once the representations of points have been found, the authors propose the following transforms in the latent space for data augmentation:
&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Hard example interpolation&lt;/h3&gt;
&lt;p&gt;For each sample in the training set, the \(q\) nearest hard examples of that class are found. Then the data point is interpolated towards the nearest hard example to get a new augmented data point.
&lt;br /&gt;
In the figure below, the data point being considered is represented by \(\vec{\,a}\). \(q1\) through \(q5\) are five hard examples of the same class as \(\vec{\,a}\) and \(q3\) (represented by \(\vec{\,b}\)) is selected as it is the nearest to \(\vec{\,a}\) among the five. \(\vec{\,a}\) is interpolated towards \(\vec{\,b}\) with a scaling factor \(\lambda\) applied.
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/HardInterFig.png&quot; alt=&quot;Illustration of hard example interpolation&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: Illustration of hard example interpolation
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Hard example extrapolation&lt;/h3&gt;
&lt;p&gt;Similar to interpolation, each sample in the training set is considered and then shifted away from the class center to get the augmented data point.
&lt;br /&gt;
In the figure, the data point being considered is represented by \(\vec{\,a}\). The class center of \(\vec{\,a}\) is represented by \(\vec{\,b}\). \(\vec{\,a}\) is extrapolated away from \(\vec{\,b}\) by a factor \(\lambda\).
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/HardExtraFig.png&quot; alt=&quot;Illustration of hard example extrapolation&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: Illustration of hard example extrapolation
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Gaussian Noise&lt;/h3&gt;
&lt;p&gt;Gaussian noise is added to data points to get augmented data. The mean of the Gaussian noise is zero and the standard deviation is calculated on a per class basis.
&lt;br /&gt;
In the figure below, \(\vec{\,a}\) is the data point being considered and \(\vec{\,b}\), \(\vec{\,c}\) and \(\vec{\,d}\) are three points to which a may be transformed after adding Gaussian noise. The Gaussian noise which is scaled by a factor \(\lambda\) and added is shown as a circle.
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/GaussianFig.png&quot; alt=&quot;Illustration of addition of Gaussian noise&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: Illustration of addition of Gaussian noise
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Difference Transform&lt;/h3&gt;
&lt;p&gt;The rationale behind the difference transform is that labels should be independent of differences in features within a class. The difference between two samples from the same class is added to a data point to get an augmented instance of data. Adding the difference in features to the point should not change its label.
&lt;br /&gt;
The figure shows \(\vec{\,a}\) as the data point being considered and \(\vec{\,b}\) and \(\vec{\,c}\) are two randomly sampled points from the same class as \(\vec{\,a}\). The difference of \(\vec{\,b}\) and \(\vec{\,c}\) is added to \(\vec{\,a}\) to get the augmented point.
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/DiffTransform.png&quot; alt=&quot;Illustration of the difference transform&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: Illustration of the difference transform
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Population Based Augmentation (PBA)&lt;/h2&gt;
&lt;p&gt;Instead of fixing the set of augmentations to be applied to latent representations at all stages of training, the authors use a technique called PBA[6] to find optimized sets of augmentations to be applied at different stages. A few terms that PBA uses are defined below.
&lt;br /&gt;
&lt;br /&gt;
An augmentation function is defined as a 3-tuple consisting of a transformation, the probability of its application and its magnitude. An example given in the paper is that (op: rotation, p = 0.4, lambda = 0.5) is an augmentation function that applies the rotation transformation with a probability of 0.4 and a magnitude of 0.5.
&lt;br /&gt;
A policy is defined as the set of all 3-tuples which are to be applied to the dataset. 0, 1 or 2 tuples are randomly sampled from the current policy and applied to each mini-batch.
&lt;br /&gt;
A policy schedule is the order of policies which will be applied at different stages of training.
&lt;br /&gt;
&lt;br /&gt;
PBA searches for a good policy schedule which will give us optimized augmentation hyperparameters to be applied at different stages of training. PBA exploits population based training which is a method in which multiple child models are trained simultaneously to learn an optimal policy schedule. At the beginning each child is assigned a randomly sampled policy from the set of all possible policies (corresponding to all possible values of probability and magnitude). The child models are initialized with random weights and are trained for a fixed number of steps following which they are evaluated using the validation set. The evaluation reveals the worst and best of the models being trained. The worst models copy the weights and policies learned by the best models. The parameters (probability and magnitude) of the copied policies are then perturbed slightly or resampled from the set of all possible values. This is done to avoid creating duplicate models. This sequence of steps is repeated until the child models are done training. In the end, the best child model is selected and its policy schedule is used.
&lt;br /&gt;
There is reason to believe that this approach for finding a good policy schedule will work. The authors of [6] state that “… though the space of schedules is large, most good schedules are necessarily smooth and hence easily discoverable through evolutionary search algorithms such as PBT.”
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/PBAFig.png&quot; alt=&quot;The sequence of steps of PBA&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: The sequence of steps of PBA
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;In their experiments the authors applied the MODALS framework on text, tabular, time-series and image data. In general the framework works well across modalities except in the imaging modality where they found that using PBA with image specific data augmentation techniques performed better than MODALS. While MODALS is a powerful technique for data augmentation, domain specific augmentation techniques could still be used in certain contexts like image data. Domain knowledge and insights about the data can be incorporated through handcrafted augmentations but not in MODALS. As the authors themselves note, one possible approach is to use a combination of both, using handcrafted augmentations to incorporate domain knowledge and insights into the augmentation.
&lt;br /&gt;
&lt;br /&gt;
The framework proposed in the paper builds around classification tasks where each data point has a single label. There is potential for a technique like MODALS to be extended to other tasks like regression and multi-label classification.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Cheung, Tsz-Him, and Dit-Yan Yeung. “MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space.” International Conference on Learning Representations. 2020.
&lt;br /&gt;
[2] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
&lt;br /&gt;
[3] Yun, Sangdoo, et al. “Cutmix: Regularization strategy to train strong classifiers with localizable features.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
&lt;br /&gt;
[4] Liu, Zicheng, et al. “AutoMix: Unveiling the Power of Mixup.” arXiv preprint arXiv:2103.13027 (2021).
&lt;br /&gt;
[5] DeVries, Terrance, and Graham W. Taylor. “Dataset augmentation in feature space.” arXiv preprint arXiv:1702.05538 (2017).
&lt;br /&gt;
[6] Ho, Daniel, et al. “Population based augmentation: Efficient learning of augmentation policy schedules.” International Conference on Machine Learning. PMLR, 2019.
&lt;br /&gt;
&lt;br /&gt;
Images created using: The Manim Community Developers. (2022). Manim – Mathematical Animation Framework (Version v0.14.0) [Computer software]. https://www.manim.community/&lt;/p&gt;</content><author><name></name></author><category term="deep learning" /><category term="data augmentation" /><category term="latent space" /><category term="data modalities" /><category term="automated data augmentation" /><summary type="html">In this post we discuss the paper “MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space”[1] Data augmentation is important for all types of deep learning tasks: unsupervised, supervised and self-supervised. Data augmentation has almost become a prerequisite for deep learning. In fact many self-supervised learning tasks [2] use data augmentation as a core component of the learning algorithm. It is also heavily used in image models and has consistently produced good results. By perturbing the data during training we are able to train more robust models, especially when there is limited data. Data augmentation is also very useful in incorporating known priors or domain knowledge into the model. For example the random flip transform applied to image data implicitly encodes the prior knowledge we have that the data is invariant to changes in orientation. The usefulness of data augmentation has led to the development of specific techniques of augmentation unique to each modality of data. The techniques developed for one modality usually suit the type of data in that particular modality. For image data some commonly used augmentation techniques are rotation, cropping, applying affine transforms, random flips, contrast and color augmentations and adding Gaussian blur. More recent techniques like CutMix[3] and MixUp[4] apply data augmentation on both the image and label space. Many robust data augmentation techniques for image data already exist and are used widely. However modalities like tables and graph data don’t have as many robust augmentation techniques. Since the techniques developed for images were made taking into consideration the nature of image data, they usually cannot be applied to other modalities. If a generalized, modality-agnostic framework for augmentation could be developed then standard, robust augmentation techniques can be applied across many modalities. This is exactly what the authors of the paper propose. In order to be independent of modality, such a framework would have to convert data from different modalities to a common representation on which label-preserving transforms can be applied. The authors point out that there exist techniques of data augmentation that use autoencoders to learn a latent space representation [5]. This is then used to generate augmentation data for the downstream task. In the framework proposed by the paper the learning of the latent space is integrated with the downstream task. A model is trained for the downstream task and an intermediate layer encodes the representations of the input data in the latent space. The framework uses interpolation and extrapolation of existing data points to produce augmented data. This requires that the latent space learned must be smooth (the representations cannot vary abruptly for a minute change in input features) for transformations to be label preserving and produce valid data. The smoothness ensures that the augmented data is inside the data manifold (ensuring it is valid data) and has the same label as the points used to generate it. To ensure this property the authors add two additional losses to the encoder used to learn the latent space. These are the adversarial loss and the triplet loss. Adversarial Loss To achieve a smooth latent space data manifold we use a discriminator network and add an adversarial loss to the model. The discriminator, given a data point, tries to predict if it is from the latent space or is a randomly sampled point from a Gaussian distribution. To keep the adversarial loss low (to “fool” the discriminator network) the model must also learn a smooth data representation so as to be indistinguishable from a smooth Gaussian distribution. Triplet Loss The aim is to have the representations of all the data points of a class to be close to each other and far away from data points from other classes. To learn the latent space in this way we add the triplet loss to the model. The Triplet loss pulls together samples from the same class and pushes away samples from other classes. Figure: The MODALS framework (figure from paper) Transformations in the latent space Once the representations of points have been found, the authors propose the following transforms in the latent space for data augmentation: Hard example interpolation For each sample in the training set, the \(q\) nearest hard examples of that class are found. Then the data point is interpolated towards the nearest hard example to get a new augmented data point. In the figure below, the data point being considered is represented by \(\vec{\,a}\). \(q1\) through \(q5\) are five hard examples of the same class as \(\vec{\,a}\) and \(q3\) (represented by \(\vec{\,b}\)) is selected as it is the nearest to \(\vec{\,a}\) among the five. \(\vec{\,a}\) is interpolated towards \(\vec{\,b}\) with a scaling factor \(\lambda\) applied. Figure: Illustration of hard example interpolation Hard example extrapolation Similar to interpolation, each sample in the training set is considered and then shifted away from the class center to get the augmented data point. In the figure, the data point being considered is represented by \(\vec{\,a}\). The class center of \(\vec{\,a}\) is represented by \(\vec{\,b}\). \(\vec{\,a}\) is extrapolated away from \(\vec{\,b}\) by a factor \(\lambda\). Figure: Illustration of hard example extrapolation Gaussian Noise Gaussian noise is added to data points to get augmented data. The mean of the Gaussian noise is zero and the standard deviation is calculated on a per class basis. In the figure below, \(\vec{\,a}\) is the data point being considered and \(\vec{\,b}\), \(\vec{\,c}\) and \(\vec{\,d}\) are three points to which a may be transformed after adding Gaussian noise. The Gaussian noise which is scaled by a factor \(\lambda\) and added is shown as a circle. Figure: Illustration of addition of Gaussian noise Difference Transform The rationale behind the difference transform is that labels should be independent of differences in features within a class. The difference between two samples from the same class is added to a data point to get an augmented instance of data. Adding the difference in features to the point should not change its label. The figure shows \(\vec{\,a}\) as the data point being considered and \(\vec{\,b}\) and \(\vec{\,c}\) are two randomly sampled points from the same class as \(\vec{\,a}\). The difference of \(\vec{\,b}\) and \(\vec{\,c}\) is added to \(\vec{\,a}\) to get the augmented point. Figure: Illustration of the difference transform Population Based Augmentation (PBA) Instead of fixing the set of augmentations to be applied to latent representations at all stages of training, the authors use a technique called PBA[6] to find optimized sets of augmentations to be applied at different stages. A few terms that PBA uses are defined below. An augmentation function is defined as a 3-tuple consisting of a transformation, the probability of its application and its magnitude. An example given in the paper is that (op: rotation, p = 0.4, lambda = 0.5) is an augmentation function that applies the rotation transformation with a probability of 0.4 and a magnitude of 0.5. A policy is defined as the set of all 3-tuples which are to be applied to the dataset. 0, 1 or 2 tuples are randomly sampled from the current policy and applied to each mini-batch. A policy schedule is the order of policies which will be applied at different stages of training. PBA searches for a good policy schedule which will give us optimized augmentation hyperparameters to be applied at different stages of training. PBA exploits population based training which is a method in which multiple child models are trained simultaneously to learn an optimal policy schedule. At the beginning each child is assigned a randomly sampled policy from the set of all possible policies (corresponding to all possible values of probability and magnitude). The child models are initialized with random weights and are trained for a fixed number of steps following which they are evaluated using the validation set. The evaluation reveals the worst and best of the models being trained. The worst models copy the weights and policies learned by the best models. The parameters (probability and magnitude) of the copied policies are then perturbed slightly or resampled from the set of all possible values. This is done to avoid creating duplicate models. This sequence of steps is repeated until the child models are done training. In the end, the best child model is selected and its policy schedule is used. There is reason to believe that this approach for finding a good policy schedule will work. The authors of [6] state that “… though the space of schedules is large, most good schedules are necessarily smooth and hence easily discoverable through evolutionary search algorithms such as PBT.” Figure: The sequence of steps of PBA Closing Thoughts In their experiments the authors applied the MODALS framework on text, tabular, time-series and image data. In general the framework works well across modalities except in the imaging modality where they found that using PBA with image specific data augmentation techniques performed better than MODALS. While MODALS is a powerful technique for data augmentation, domain specific augmentation techniques could still be used in certain contexts like image data. Domain knowledge and insights about the data can be incorporated through handcrafted augmentations but not in MODALS. As the authors themselves note, one possible approach is to use a combination of both, using handcrafted augmentations to incorporate domain knowledge and insights into the augmentation. The framework proposed in the paper builds around classification tasks where each data point has a single label. There is potential for a technique like MODALS to be extended to other tasks like regression and multi-label classification. References [1] Cheung, Tsz-Him, and Dit-Yan Yeung. “MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space.” International Conference on Learning Representations. 2020. [2] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. [3] Yun, Sangdoo, et al. “Cutmix: Regularization strategy to train strong classifiers with localizable features.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019. [4] Liu, Zicheng, et al. “AutoMix: Unveiling the Power of Mixup.” arXiv preprint arXiv:2103.13027 (2021). [5] DeVries, Terrance, and Graham W. Taylor. “Dataset augmentation in feature space.” arXiv preprint arXiv:1702.05538 (2017). [6] Ho, Daniel, et al. “Population based augmentation: Efficient learning of augmentation policy schedules.” International Conference on Machine Learning. PMLR, 2019. Images created using: The Manim Community Developers. (2022). Manim – Mathematical Animation Framework (Version v0.14.0) [Computer software]. https://www.manim.community/</summary></entry></feed>