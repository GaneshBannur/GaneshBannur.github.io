<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-26T19:46:17-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ganesh Bannur</title><subtitle>Ganesh Bannur</subtitle><entry><title type="html">The (lack of) constraints on MJRTY</title><link href="http://localhost:4000/2023/11/14/mjrty.html" rel="alternate" type="text/html" title="The (lack of) constraints on MJRTY" /><published>2023-11-14T00:00:00-08:00</published><updated>2023-11-14T00:00:00-08:00</updated><id>http://localhost:4000/2023/11/14/mjrty</id><content type="html" xml:base="http://localhost:4000/2023/11/14/mjrty.html">&lt;p&gt;The MJRTY algorithm, proposed in \([1]\), is a popular majority voting algorithm that runs in linear time. In this article I‚Äôm not going to focus on the algorithm per se but rather on the constraints that are imposed on the algorithm. Specifically I‚Äôm going to offer an explanation for why one of the constraints on MJRTY can be removed (the authors mention this but offer no explanation). So read through the paper (&lt;a href=&quot;https://www.cs.utexas.edu/users/moore/best-ideas/mjrty&quot;&gt;&lt;i&gt;here&lt;/i&gt;&lt;/a&gt;) and come back.&lt;/p&gt;

&lt;p&gt;The paper uses a theorem-prover to prove the correctness of the Fortran implementation of MJRTY. To feed into the theorem-prover, they define the constraints imposed on the algorithm. Two constraints are of interest to us: Constraint \((3)\) and Constraint \((5)\).&lt;/p&gt;

&lt;p&gt;A few preliminaries&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(A\) is the array of elements whose majority element we want&lt;/li&gt;
  &lt;li&gt;\(i\) is the current index we are looking at&lt;/li&gt;
  &lt;li&gt;\(cand\) is the majority element in \(A[1...i]\) (or just some element if \(A[1...i]\) doesn‚Äôt have a majority)&lt;/li&gt;
  &lt;li&gt;\(k\) is the number of instances of \(cand\) in \(A[1...i]\) which remain after pairing up dissimilar elements in \(A[1...i]\) and cancelling them out, according to the MJRTY algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Constraint \((3)\)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The number of times \(cand\) occurs in \(A\) from \(1\) through \(i\) is at least \(k\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Constraint \((3)\) says that there are at least \(k\) instances of \(cand\) in \(A[1...i]\), i.e., \(k\) is either an underestimation or correct estimation of the number of instances of \(cand\). It is an underestimation when some instances of \(cand\) have been paired up with a dissimilar element and have not contributed to \(k\).&lt;/p&gt;

&lt;p&gt;Constraint \((5)\)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For all \(x\) other than \(cand\), the number of times \(x\) occurs in \(A\) from \(1\) through \(i\) is no greater than \(\frac{(i-k)}{2}\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Constraint \((5)\) follows obviously when we split the elements of \(A[1...i]\) into two groups (as they do in \([1]\)), the ‚Äúunanimous group‚Äù and the ‚Äúmajority-free group‚Äù. The unanimous group has the \(k\) instances of \(cand\) which haven‚Äôt been paired up (so they unanimously vote for \(cand\)) and the majority-free group has the remaining elements. There are \(i-k\) elements in the majority-free group and since each element here must be paired up with a dissimilar element, there can be no more than \(\frac{(i-k)}{2}\) elements of the same kind.&lt;/p&gt;

&lt;p&gt;We can rephrase constraint \((5)\) as&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Let \(s\) be the element, other than \(cand\), which has the most number of instances in \(A[1...i]\) (if \(cand\) is the majority element of \(A[1...i]\) then \(s\) will be the element with the second most number of instances in \(A[1...i]\), otherwise \(s\) will be the element with the most instances). The number of instances of \(s\) is \(\le \frac{(i-k)}{2}\). If there are two or more elements which satisfy the condition for \(s\) then this must apply to all of them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This phrasing is equivalent to constraint \((5)\) since, by the definition of \(s\), it has more instances than any other element in \(A[1...i]\) (leaving out \(cand\)). If s has less than \(\frac{(i-k)}{2}\) instances so will the others. From here on we use this rephrasing rather than the original constraint.&lt;/p&gt;

&lt;p&gt;\([1]\) provides a Fortran implementation of MJRTY in which they perform one small optimization over the algorithm. Once the array has been scanned and the so-called ‚Äúpairing‚Äù phase is done, the program checks the value of \(k\) to see if it is \(&amp;gt;\frac{n}{2}\). If it is then they skip the ‚Äúcounting‚Äù phase since \(k\), a possible underestimation of the number of instances of \(cand\), already tells us that there are at least \(\frac{n}{2}+1\) instances of \(cand\). This trick is important later.&lt;/p&gt;

&lt;p&gt;Using the constraints defined, \([1]\) proves that the Fortran implementation of MJRTY is correct. However they state this&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Nevertheless, if one modifies the code so that \(k\) is not tested against \(\frac{n}{2}\) before entering the counting phase, one can omit conjunct \((3)\) of this invariant. That is, unless the program exits early when \(k\) exceeds \(\frac{n}{2}\), a demon within the first loop is permitted to raise \(k\) above the count of \(cand\) (within the constraint imposed by \((5)\)) without causing the algorithm to perform incorrectly. We do not know how to interpret this lack of constraint.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Even the authors can‚Äôt tell us why constraint \((3)\) can be removed while adhering to constraint \((5)\). This demon must know something we don‚Äôt.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;interpreting&quot;&gt;Interpreting&lt;/h1&gt;
&lt;p&gt;One thing we need to get out of the way is why \(k\) should not be tested against \(\frac{n}{2}\) if we wish to remove constraint \((3)\). This is because normally there are at least \(k\) instances of \(cand\). If \(k&amp;gt;\frac{n}{2}\) then there must be at least \(\frac{n}{2}+1\) instances of \(cand\) so it is guaranteed to be the majority element and we don‚Äôt have to perform the counting phase to verify this (the ‚Äútrick‚Äù we discussed before). But if we raise the value of \(k\) (without affecting the result of the algorithm) then we can no longer be sure that there are at least \(k\) instances of \(cand\) and so we must never skip the counting phase.&lt;/p&gt;

&lt;p&gt;Now onto the real deal. Why can we remove constraint \((3)\) while adhering to constraint \((5)\)?&lt;/p&gt;

&lt;p&gt;While the algorithm is executing and has just examined position \(i\) in \(A\), \(cand\) either&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;holds the majority in element in \(A[1...i]\) if \(A[1...i]\) has a majority element&lt;/li&gt;
  &lt;li&gt;holds some element if \(A[1...i]\) does not have a majority element&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(m\) be the majority element in \(A\)&lt;/li&gt;
  &lt;li&gt;\(s\) be the element which we defined in the rephrasing of constraint \((5)\).&lt;/li&gt;
  &lt;li&gt;\(N(x)\) denote the number of instances of \(x\) &lt;span style=&quot;color:brown&quot;&gt;&lt;strong&gt;&lt;em&gt;in \(A[1...i]\)&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We say an array \(B\) is &lt;em&gt;equivalent in terms of majority&lt;/em&gt; to the array \(A\) if \(B\) has the same majority element as \(A\) (or no majority element if \(A\) has none).&lt;br /&gt;
At any point of the algorithm we can create an array \(B\) by replacing all elements in \(A[1...i]\), other than \(m\), by \(cand\). Created this way, \(B\) will be &lt;em&gt;equivalent in terms of majority&lt;/em&gt; to \(A\) since we are not replacing any instances of \(m\) and \(m\) still remains the majority element. Let \(k&apos;\) be the value of \(k\) we would get by applying MJRTY on \(B[1...i]\). Since we are replacing other elements with \(cand\) in \(A[1...i]\) to get \(B[1...i]\) we have \(k&apos;\ge k\). They will be equal if there are no elements in \(A[1...i]\) other than \(m\) and \(cand\).&lt;/p&gt;

&lt;p&gt;Now we can let the algorithm continue its execution from position \(i\) in \(B\) and it will yield \(m\) itself as the majority element. Converting \(A\) to \(B\) and continuing on \(B\) is equivalent to simply raising the value of \(k\) to \(k&apos;\) and continuing on \(A\). This is why the value of \(k\) can be raised without affecting the result of the algorithm. Depending on how many replacements we can perform, \(k&apos;\) may be more than \(N(cand)\) in which case constraint \((3)\) would need to be removed to allow this.&lt;/p&gt;

&lt;p&gt;This method may actually let us raise the value of \(k\) by more than what removing constraint \((3)\) while adhering to constraint \((5)\), will allow. To see why let‚Äôs consider the two possibilities&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\(s=m\)&lt;br /&gt;
In this case the method I have described degenerates to removing constraint \((3)\). Consider the array \(B\) that we constructed. In \(B[1...i]\) we pair each instance of \(m\) with an instance of \(cand\) and the remaining instances of \(cand\) contribute to \(k&apos;\). \(N(m) = \frac{(i-k&apos;)}{2}\) since there are \(i-k&apos;\) paired elements and half of them will be instances of \(m\). So we are adhering to constraint \((5)\) since \(N(s) = \frac{(i-k&apos;)}{2} =&amp;gt; N(s) \le \frac{(i-k)}{2}\). We are able to increase \(k\) to \(k&apos; = i - 2 \times N(s)\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(s \ne m\)&lt;br /&gt;
Considering array \(B\) again, we pair each instance of \(m\) with an instance of \(cand\) and the remaining instances of \(cand\) contribute to \(k&apos;\). However this time we are not adhering to constraint \((5)\) since we are replacing \(s\) by \(cand\). Since, apart from \(cand\), \(B\) only has instances of \(m\), we require that \(m\) (and not \(s\)) must pair with instances of \(cand\). And since \(N(m) \le N(s)\) (equal if \(m\) and the element chosen for \(s\) have the same number of instances) and \(N(m) = \frac{(i-k&apos;)}{2}\) we get \(N(s) \ge \frac{(i-k&apos;)}{2}\). So \(N(s)\) may be \(&amp;gt;\frac{(i-k)}{2}\) and we may violate constraint \((5)\). In this case we have increased \(k\) by more than if we had adhered to constraint \((5)\). We are able to increase \(k\) to \(k&apos; = i - 2 \times N(m)\). Adhering to constraint \((5)\) would have let us increase \(k\) to just \(k&apos; = i - 2 \times N(s)\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In either case we get \(k&apos; = i - 2 \times N(m)\). Removing constraint \((3)\) permits us to raise \(k\) above \(N(cand)\). But this can only happen when \(k&apos;&amp;gt;N(cand) =&amp;gt; i - 2 \times N(m)&amp;gt;N(cand) =&amp;gt; N(m) &amp;lt; \frac{i - N(cand)}{2}\).&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Note: Sometimes we can do even better than this (click to reveal)&lt;/summary&gt;
&lt;br /&gt;
We can increase \(k\) past the previous method if we know \(A[i+1...n]\). In this case we can even replace some instances of \(m\) with \(cand\) in \(B[1...i]\). We just have to make sure that \(B[i+1...n]\) has the correct elements to give the right answer. For example:  
$$A:\ [a\ e\ c\ e\ f\ |\ e\ e\ e\ b\ e\ e\ e\ d]\quad i = 5,\ cand = f,\ k = 1,\ m = e$$
We can replace everything in \(A[1...5]\), even \(e\), to get  
$$B:\ [f\ f\ f\ f\ f\ |\ e\ e\ e\ b\ e\ e\ e\ d]\quad i = 5,\ cand = f,\ k&apos; = 5, m = e$$  
Continuing from \(B\) will still give us \(e\) as the final result even though we replaced \(e\) with \(f\). Note that it is not just the number of instances of \(m\) in \(B[i+1...n]\) but also the order that we have to take into account. For example if we had  
$$A:\ [a\ e\ c\ e\ f\ |\ e\ e\ e\ e\ e\ b\ e\ d]\quad i = 5,\ cand = f,\ k = 1,\ m = e$$  
Replacing we would get  
$$B:\ [f\ f\ f\ f\ f\ |\ e\ e\ e\ e\ e\ b\ e\ d]\quad i = 5,\ cand = f,\ k&apos; = 5,\ m = e$$  
But this would give the wrong final result, \(d\).  
So this method is finicky and requires us to know \(A[i+1...n]\) completely. But the method described earlier only requires us to know \(m\) and does not need any information on \(A[i+1...n]\).
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;we-did-it--right&quot;&gt;We Did It! ‚Ä¶.. Right?&lt;/h1&gt;
&lt;p&gt;We‚Äôve managed to do better than what the paper tells us we can by removing constraint \((3)\). We did it!&lt;br /&gt;
There‚Äôs a &lt;strong&gt;&lt;em&gt;tiny&lt;/em&gt;&lt;/strong&gt; problem:&lt;br /&gt;
We assumed that we know the majority element \(m\), the very thing the algorithm is trying to find! üòë&lt;/p&gt;

&lt;p&gt;In reality we don‚Äôt know \(m\) and so we can‚Äôt calculate \(k&apos; = i - 2 \times N(m)\). Raising \(k\) to \(k&apos; = i - 2 \times N(m)\) would have allowed us to continue the algorithm without changing the result. But in the absence of \(N(m)\) the best we can do is to raise \(k\) to \(k&apos; = i - 2 \times N(s)\). This is guaranteed to not change the result since \(N(m) \le N(s)\) and so \(i - 2 \times N(m) \ge i - 2 \times N(s)\), i.e., if raising to \(i - 2 \times N(m)\) won‚Äôt affect the result, raising it to a smaller value (\(i - 2 \times N(s)\)) certainly won‚Äôt. Raising \(k\) to \(k&apos; = i - 2 \times N(s)\) gives \(N(s) = \frac{(i-k&apos;)}{2}\) and so constraint \((5)\) is satisfied even after raising \(k\) to \(k&apos;\).&lt;br /&gt;
Finally, removing constraint \((3)\) permits us to raise \(k\) above \(N(cand)\). But this can only happen when \(k&apos;&amp;gt;N(cand) =&amp;gt; i - 2 \times N(s)&amp;gt;N(cand) =&amp;gt; N(s) &amp;lt; \frac{i - N(cand)}{2}\).&lt;/p&gt;

&lt;p&gt;We now know why we can remove constraint \((3)\) by raising \(k\) while adhering to constraint \((5)\). The demon‚Äôs got nothing on us.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
If you see any issues please let me know by filing an issue at &lt;a href=&quot;https://github.com/GaneshBannur/ganeshbannur.github.io/issues&quot;&gt;https://github.com/GaneshBannur/ganeshbannur.github.io/issues&lt;/a&gt;.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;\([1]\) Boyer, Robert S., and J. Strother Moore. ‚ÄúMJRTY‚Äîa fast majority vote algorithm.‚Äù Automated reasoning: essays in honor of Woody Bledsoe. Dordrecht: Springer Netherlands, 1991. 105-117.&lt;/p&gt;</content><author><name></name></author><summary type="html">The MJRTY algorithm, proposed in \([1]\), is a popular majority voting algorithm that runs in linear time. In this article I‚Äôm not going to focus on the algorithm per se but rather on the constraints that are imposed on the algorithm. Specifically I‚Äôm going to offer an explanation for why one of the constraints on MJRTY can be removed (the authors mention this but offer no explanation). So read through the paper (here) and come back.</summary></entry><entry><title type="html">A fast fuzzy substring algorithm</title><link href="http://localhost:4000/2023/09/03/fuzzy-substring.html" rel="alternate" type="text/html" title="A fast fuzzy substring algorithm" /><published>2023-09-03T00:00:00-07:00</published><updated>2023-09-03T00:00:00-07:00</updated><id>http://localhost:4000/2023/09/03/fuzzy-substring</id><content type="html" xml:base="http://localhost:4000/2023/09/03/fuzzy-substring.html">&lt;p&gt;This article is about an algorithm I came up with for fast fuzzy substring search, i.e., searching for the best match substring of a query in a corpus. Given a query string and a corpus string, the algorithm tries to find the substring in the corpus which is the most similar to the query.&lt;br /&gt;
The code implementing the algorithm is at &lt;a href=&quot;https://github.com/GaneshBannur/fuzzy-substring&quot;&gt;https://github.com/GaneshBannur/fuzzy-substring&lt;/a&gt;.&lt;br /&gt;
This article is an extension of the Stackoverflow answer I gave &lt;a href=&quot;https://stackoverflow.com/a/77019582&quot;&gt;https://stackoverflow.com/a/77019582&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A naive approach would be to go through every substring of the corpus and find the most similar one, however this would be too slow for large corpora. Instead the algorithm focuses on speed while providing the optimal result in most cases. It is not possible to guarantee that the result is optimal without some level of brute forcing, which slows down the search significantly. This is because at some point (after possibly narrowing down the search region in the corpus) such an algorithm would have to check every substring. Hence this algorithm does not provide that guarantee but it has been seen empirically that it provides the optimal result in most cases. At some point I‚Äôll add the theoretical characterization of the failure cases.&lt;/p&gt;

&lt;p&gt;The first step is to define what it means for a substring of the corpus to be similar to the query. Any one of the plethora of string similarity/distance measures can be used in this algorithm. Once the measure is decided, the definition of the closest substring is as follows: The substring of the corpus which has the lowest distance (for distance measures) or highest similarity (for similarity measures) with the query.&lt;/p&gt;

&lt;p&gt;I tested two measures: Levenshtein distance and Ratcliff-Obershelp similarity (specifically the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ratio&lt;/code&gt; function in Python‚Äôs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;difflib&lt;/code&gt; module). Empirically Levenshtein distance was found to work best for two reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It provides more accurate results&lt;/li&gt;
  &lt;li&gt;It is extremely fast to compute&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now it is easy to define the most similar substring: The substring of the corpus most similar to the query is the one which has the lowest Levenshtein distance from it.&lt;/p&gt;

&lt;p&gt;The algorithm finds the best substring match in two stages&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;First it  performs a quick low resolution scan over the corpus to identify the general region of the corpus in which the best match may exist&lt;/li&gt;
  &lt;li&gt;Second it performs multiple high resolution scans over the region identified to try and find the exact substring that is the  best match&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Resolution here includes the step size and window size of the sliding window scan. The low resolution scan will have a high step size and high window size. The high resolution scan will have a low step size and low window size. Multiple high resolution scans with varying window sizes and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step size = 1&lt;/code&gt; are performed to be able to identify the best match exactly, regardless of its length or position in the region identified.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;first-stage-low-resolution-scan&quot;&gt;First Stage (low resolution scan)&lt;/h1&gt;
&lt;p&gt;The first stage constructs substrings of the same size as the query using a sliding window over the corpus (the sliding window moves forward by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step&lt;/code&gt; every time). The substrings constructed would be&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;corpus[0 ... query_len - 1], corpus[step ... step + query_len - 1], corpus[2*step ... 2*step + query_len - 1], .....
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The best value for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step&lt;/code&gt; is empirically found to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step = query_len//2&lt;/code&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;//&lt;/code&gt; is integer division). This value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step&lt;/code&gt; works well because it provides a balance between speed and accuracy: a smaller step size would decrease speed and a larger step size may miss regions of the corpus. The smallest step size possible is 1. It gives the best accuracy as every substring of the corpus of length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len&lt;/code&gt; is considered. It is also the slowest for the same reason. The largest step size possible is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len&lt;/code&gt;, any larger than this and the scan would miss parts of the corpus. It gives the fastest scan since it takes large steps. It also has the worst accuracy for the same reason. A step size of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len//2&lt;/code&gt; sits between the smallest and largest step sizes, giving it a balance between speed and accuracy.&lt;/p&gt;

&lt;p&gt;The substrings constructed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step = query_len//2&lt;/code&gt; are (assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len&lt;/code&gt; is divisible by two for simplicity)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;corpus[0 ... query_len - 1], corpus[query_len/2 ... 1.5*query_len - 1], corpus[query_len ... 2*query_len - 1], .....
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first stage scans over the corpus by constructing these substrings. The substring with the minimum Levenshtein distance from the query is found. This substring gives the region of the corpus around which the best match will most likely exist. The first stage serves the purpose of narrowing down the search region in the corpus. Once the region is found by the first stage it is doubled in length by considering &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len//2&lt;/code&gt; characters to both the left and right of the region. If the first stage narrowed down to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;corpus[i ... i + query_len - 1]&lt;/code&gt; then the search region for the second stage is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;corpus[i - query_len//2 ... i + query_len - 1 + query_len//2]&lt;/code&gt;. This gives us a region of length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len//2 (left extension) + query_len (first scan gives region of length = query_len) + query_len//2 (right extension) = 2*query_len&lt;/code&gt;. The region is expanded to account for the error in identifying the exact region (since the first scan was a low resolution scan that jumped over the corpus with steps of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len//2&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;second-stage-high-resolution-scans&quot;&gt;Second Stage (high resolution scans)&lt;/h1&gt;
&lt;p&gt;The second stage performs a more thorough search over the region identified (called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;narrowed_corpus&lt;/code&gt; in the code). This is done by constructing substrings of varying lengths over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;narrowed_corpus&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step = 1&lt;/code&gt;. This is equivalent to performing multiple scans over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;narrowed_corpus&lt;/code&gt;, each one with a different window size and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step = 1&lt;/code&gt; for the sliding window. Multiple scans are performed to consider more substrings of varying positions and lengths within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;narrowed_corpus&lt;/code&gt;, increasing our chances of finding the optimal solution.&lt;/p&gt;

&lt;p&gt;Now we need to decide what substring lengths to consider. To decide this, we first narrow down the range of values considered using the following result.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The best match string &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;, which has the least Levenshtein distance from the query, has length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l&lt;/code&gt; which satisfies &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l &amp;lt;= 2*query_len&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The proof of this result is as follows&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Proof&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We can show that no string of length more than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2*query_len&lt;/code&gt; can have the minimum Levenshtein distance from the query.
Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s = corpus[i ... i + l - 1]&lt;/code&gt; be the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; defined in the result.&lt;/p&gt;

&lt;p&gt;The Levenshtein distance of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; from the query is the number of insertions, deletions and substitutions required to convert &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;. This has two components&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Substitution of characters in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;, if some &lt;strong&gt;&lt;em&gt;subsequence&lt;/em&gt;&lt;/strong&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; does not match &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; exactly&lt;/li&gt;
  &lt;li&gt;Removal of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l - query_len&lt;/code&gt; extra characters from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;, since it is longer than the query&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Considering only the second part tells us that the Levenshtein distance must be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt; query_len&lt;/code&gt; since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l &amp;gt; 2*query_len =&amp;gt; l - query_len &amp;gt; query_len&lt;/code&gt;. But any string of length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;= query_len&lt;/code&gt; can be converted to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;= query_len&lt;/code&gt; insertions and substitutions. So no string of length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt; 2*query_len&lt;/code&gt; can be the best match since.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The result imposes a theoretical upper limit on the substring lengths we should consider.
We can also impose a heuristic lower limit on the substring lengths by using the result. We can consider only substrings with length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt; query_len/2&lt;/code&gt;. Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; be a substring with length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l&lt;/code&gt; such that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l &amp;lt; query_len/2&lt;/code&gt; (assume it‚Äôs divisible by 2). Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len &amp;gt; 2*l&lt;/code&gt;, the result tells us that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; cannot be the best match for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;. Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; may still be the best match for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;, it is just that there is some other substring in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;corpus&lt;/code&gt; which is a better match for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; is. But empirically we can assume that if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; is not the best match for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; is probably not the best match for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; (of course, this need not always be true). But assuming this gives us the lower bound on substring lengths as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len//2&lt;/code&gt;. Empirically this assumption is found to not affect the optimality of the result in most cases.&lt;/p&gt;

&lt;p&gt;But here‚Äôs an example of when this assumption causes the algorithm to be non-optimal:&lt;br /&gt;
Input&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;query = &quot;philanthropic&quot;
corpus = &quot;bbbbbbbbbbbbbbphiicbbbbbbbbbbbbbb&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optimal Output&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Best Match: &quot;phiic&quot;
Minimum Levenshtein distance: 8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Actual Output&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Best Match: &quot;bphiic&quot;
Minimum Levenshtein distance: 9
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Characterizing the failures because of this assumption&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Even when the best match has length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt; query_len/2&lt;/code&gt;, the assumption does not cause the algorithm to be non-optimal in one specific case: when the best match is a &lt;strong&gt;&lt;em&gt;substring&lt;/em&gt;&lt;/strong&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; and not just a &lt;strong&gt;&lt;em&gt;subsequence&lt;/em&gt;&lt;/strong&gt;. The example failure given above occurs because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;phiic&quot;&lt;/code&gt; is a subsequence of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;philanthropic&quot;&lt;/code&gt; but not a substring. This means some characters need to be filled &lt;em&gt;in between&lt;/em&gt; the characters (rather than appending extra characters to a side) of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;phiic&quot;&lt;/code&gt; to get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;phi&lt;/code&gt;&lt;em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lanthrop&lt;/code&gt;&lt;/em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ic&quot;&lt;/code&gt;. If however the best match is a substring of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; then we can find a substring with length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt; query_len/2&lt;/code&gt; that has the same Levenshtein distance.&lt;br /&gt;
Let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;, a substring of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;, be the best match with length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt; query_len/2&lt;/code&gt;. Then to convert &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; we would simply need to add the remaining characters of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; to the sides of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;. This would give us a Levenshtein distance of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len - s_len&lt;/code&gt;. As an example, consider the failure case. If instead the corpus was &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;bbbbbbbbbbbbbblanthbbbbbbbbbbbbbb&quot;&lt;/code&gt;, the best match &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;lanth&quot;&lt;/code&gt; would be a substring of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;philanthropic&quot;&lt;/code&gt; and we would only need to add characters to the sides (&lt;em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;phi&lt;/code&gt;&lt;/em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lanth&lt;/code&gt;&lt;em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ropic&quot;&lt;/code&gt;&lt;/em&gt;).&lt;br /&gt;
Suppose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; characters need to be added to the left of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; characters to the right to convert it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;. In this case we can find another substring of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;corpus&lt;/code&gt; which adds &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; characters of &lt;strong&gt;&lt;em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;corpus&lt;/code&gt;&lt;/em&gt;&lt;/strong&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;. If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s = corpus[i ... i + s_len - 1]&lt;/code&gt; then the new substring would be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&apos; = corpus[i - n ... i + s_len - 1 + m]&lt;/code&gt; (in our example &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&apos; = &quot;&lt;/code&gt;&lt;em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bbb&lt;/code&gt;&lt;/em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lanth&lt;/code&gt;&lt;em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bbbbb&lt;/code&gt;&lt;/em&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;&lt;/code&gt;). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&apos;&lt;/code&gt; would have the same Levenshtein distance as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; since instead of adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; characters to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;, we can instead substitute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; extra characters in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&apos;&lt;/code&gt; to get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;. But we could not have done the same if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; was a subsequence and not a substring of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; and required insertions &lt;em&gt;in between&lt;/em&gt; characters.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In light of this, the second stage does not consider any substring of length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt; query_len//2&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt; 2*query_len&lt;/code&gt;. So the lengths of substrings considered for the multiple high resolution scans range from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len//2&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2*query_len&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We have established the range of substring lengths that we will consider. The next step is to decide which substring lengths to consider. The second stage considers substring lengths at fixed intervals between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len//2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2*query_len&lt;/code&gt;. The intervals are defined by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step = query_len//step_factor&lt;/code&gt;. The lengths of substrings constructed depend on the argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step_factor&lt;/code&gt;. The lengths considered are&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;len(query)//2 - 1, len(query)//2 - 1 + step, len(query)//2 - 1 + 2*step, ....., 2 * len(query) + 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The second stage considers lengths at intervals as considering all lengths between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query_len//2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2*query_len&lt;/code&gt; would slow it down significantly.&lt;br /&gt;
Empirically &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step_factor = 128&lt;/code&gt; works well, compromising between speed and chances of an optimal solution. Using this value, the lengths considered are&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;len(query)//2 - 1, len(query)//2 - 1 + query_len//128, len(query)//2 - 1 + 2*query_len//128, ....., 2 * len(query) + 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Increasing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step_factor&lt;/code&gt; increases the number of substring lengths considered hence increasing the chances of finding the optimal substring with minimum Levenshtein distance. However increasing it will lead to increased runtime because more substrings need to be checked.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A final consideration in the algorithm is the preference for smaller or larger matches. Among all the substrings that the algorithm ends up searching, more than one may have minimum Levenshtein distance. The algorithm can return a list of all the substrings which have the same distance (the minimum distance). However when implementing the algorithm practically it is nice to return one substring as the best match. In this case we have to choose which substring from the list we return. Empirically it is found to be better to prefer the largest match. Perhaps because it is easier for people to skip over extraneous characters in a larger match than fill in missing characters in a smaller match.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;an-example-of-the-algorithm&quot;&gt;An example of the algorithm&lt;/h1&gt;

&lt;p&gt;Input&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;query = &quot;ipsum dolor&quot;
corpus = &quot;lorem 1psum dlr sit amet&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Output&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Best match: &quot;1psum dlr&quot;
Minimum Levenshtein distance: 3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;example-that-breaks-the-algorithm&quot;&gt;Example that breaks the algorithm&lt;/h1&gt;
&lt;p&gt;This is an example which misdirects the first stage of the algorithm and causes it to select the wrong region, making the end result of the algorithm entirely wrong.&lt;/p&gt;

&lt;p&gt;Input (note the extra spaces)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;query: &quot;antidisestablishmentarianism&quot;
corpus: &quot;    anidietabishetariism    antidbmjrietabishetariism   &quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optimal Output&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Best match: &quot;anidietabishetariism&quot;
Minimum Levenshtein distance: 8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Actual Output&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Best match: &quot;antidbmjrietabishetariism&quot;
Minimum Levenshtein distance: 11
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">This article is about an algorithm I came up with for fast fuzzy substring search, i.e., searching for the best match substring of a query in a corpus. Given a query string and a corpus string, the algorithm tries to find the substring in the corpus which is the most similar to the query. The code implementing the algorithm is at https://github.com/GaneshBannur/fuzzy-substring. This article is an extension of the Stackoverflow answer I gave https://stackoverflow.com/a/77019582.</summary></entry><entry><title type="html">Survey of text detection and recognition</title><link href="http://localhost:4000/2023/05/28/text-survey.html" rel="alternate" type="text/html" title="Survey of text detection and recognition" /><published>2023-05-28T00:00:00-07:00</published><updated>2023-05-28T00:00:00-07:00</updated><id>http://localhost:4000/2023/05/28/text-survey</id><content type="html" xml:base="http://localhost:4000/2023/05/28/text-survey.html">&lt;p&gt;This article is a brief survey of the techniques in text detection and recognition.&lt;/p&gt;

&lt;p&gt;Extracting text from images, called text spotting, is arguably the most important technique for extracting information from images. This is because text is the most dense representation of information present in images. Real-world scenes are rich in text and identifying the text present in a scene can vastly improve the understanding of the scene.&lt;/p&gt;

&lt;p&gt;Traditionally text spotting has been divided into text detection and text recognition, both performed by separate methods. This is true even of recent deep learning methods. There are dedicated text detection and recognition models. Recently there has been an increased effort to train end-to-end text spotting models, which are single models that recognize the text in an image in one shot. These methods are expected to have better results due to information sharing between the tasks of detection and recognition. While these end-to-end methods are improving they are still behind pipelined methods which break the task down into detection and recognition. This is evidenced by [&lt;strong&gt;1&lt;/strong&gt;], in which they state&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It‚Äôs widely believed that end-to-end models enjoy shared feature extraction which leads to better accuracy. However, the results of our competition say otherwise.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In that spirit the rest of this article gives surveys and comparisons of text detection and recognition techniques. However, keep in mind that only papers with official code implementations were considered since I was surveying techniques to integrate them into a pipeline.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;detection&quot;&gt;Detection&lt;/h1&gt;
&lt;p&gt;Here is a comparison of text detection techniques. For each technique the result tables from the corresponding paper are given. The first table condenses the results of all the techniques. There is also a reference image given below the table which is used to demonstrate the output of the techniques. I ultimately chose to use the &lt;em&gt;unified detector&lt;/em&gt; from [&lt;strong&gt;2&lt;/strong&gt;] since it produced the best results empirically. However the &lt;em&gt;boundary transformer&lt;/em&gt; from [&lt;strong&gt;3&lt;/strong&gt;] is the best on paper and may be better in other scenarios.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/text-survey/Text-Detection.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/text-survey/Text-Detection.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;recognition&quot;&gt;Recognition&lt;/h1&gt;
&lt;p&gt;Here is a survey of text recognition techniques. The result tables for some interesting techniques are given.&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/text-survey/text-recognition.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/text-survey/text-recognition.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Long, Shangbang, et al. ‚ÄúICDAR 2023 Competition on Hierarchical Text Detection and Recognition.‚Äù arXiv preprint arXiv:2305.09750 (2023).&lt;br /&gt;
[2] Long, Shangbang, et al. ‚ÄúTowards end-to-end unified scene text detection and layout analysis.‚Äù Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.&lt;br /&gt;
[3] Zhang, Shi-Xue, et al. ‚ÄúArbitrary shape text detection via boundary transformer.‚Äù IEEE Transactions on Multimedia (2023).&lt;/p&gt;</content><author><name></name></author><summary type="html">This article is a brief survey of the techniques in text detection and recognition.</summary></entry><entry><title type="html">Survey of vision and vision-language tasks for robots</title><link href="http://localhost:4000/2023/01/10/v-and-vl-survey.html" rel="alternate" type="text/html" title="Survey of vision and vision-language tasks for robots" /><published>2023-01-10T00:00:00-08:00</published><updated>2023-01-10T00:00:00-08:00</updated><id>http://localhost:4000/2023/01/10/v-and-vl-survey</id><content type="html" xml:base="http://localhost:4000/2023/01/10/v-and-vl-survey.html">&lt;p&gt;This is a survey of techniques and datasets for some vision and vision-language tasks. The survey was done on tasks which could be useful to a robot. A variety of tasks are explored:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Extraction of relevant information from the surrounding scene&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  This is done through the robot‚Äôs vision pipeline. Different extraction methods are applied on the robot‚Äôs video stream to extract different types of information from the scene. Object detection, event detection, human action detection and text detection and recognition are some of the key extraction methods applied. Text detection and recognition techniques are explored in more detail &lt;a href=&quot;/2023/01/10/v-and-vl-survey.html&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
  Retrieving extracted information based on natural language inputs provided by humans is also very important for a robot. Some techniques to retrieve objects in the scene based on their description are also explored.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Navigation&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  Navigation is a fundamental task for a robot. Recent techniques have explored navigation as a vision-language task in which the robot must navigate based on natural language commands and its video stream.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Gaining visual knowledge&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  In order to perform generic tasks such as zero-shot navigation in a new environment or visual-question answering based on the current scene, a robot must have large scale visual knowledge. A robot can either query an external knowledge base or accumulate knowledge in the parameters of a deep learning model. Recent work has explored the latter since end-to-end models with large scale visual knowledge perform better at visual tasks. Some papers proposing frameworks for large scale visual knowledge acquisition are explored.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Miscellaneous&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  Other interesting papers on datasets, custom architectures for video tasks, pretraining and finding specific moments in video are also surveyed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Papers proposing techniques for these categories are listed. The key ideas put forth by papers in each category are summarized.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/v-vl-survey/Vision-Query-Literature-Survey.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/v-vl-survey/Vision-Query-Literature-Survey.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Here is a survey of datasets available for the tasks mentioned above.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/v-vl-survey/Vision-Query-Datasets.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/v-vl-survey/Vision-Query-Datasets.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;predicting-the-future&quot;&gt;Predicting the future&lt;/h1&gt;
&lt;p&gt;Humans are able to predict the future. Well, at least in the short term. We are able to judge facial expressions and predict what a person may do, observe movements and predict where a person is headed and in general predict future actions based on familiar visual cues received till now. Predicting the intent or next action of a human based on current and past video frames is crucial for robots to be proactive rather than reactive when interacting with humans. Predicting the short term (or even better - long term) goals of a person will let a robot take preemptive action. Current work focuses on video-based prediction as it is easy to acquire a video feed from a robot. [&lt;strong&gt;1&lt;/strong&gt;] justifies this by showing that video-based prediction is not inferior to using 3D kinematics, which is able to capture every dimension of human motion.&lt;/p&gt;

&lt;p&gt;Here is a survey of techniques that have been used for predicting human actions and intents&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/v-vl-survey/Vision-Based-Intent.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/v-vl-survey/Vision-Based-Intent.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;And here are some datasets that exist for this task&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/v-vl-survey/Vision-Based-Intent-Datasets.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/v-vl-survey/Vision-Based-Intent-Datasets.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Zunino, Andrea, et al. ‚ÄúWhat will i do next? The intention from motion experiment.‚Äù Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017.&lt;/p&gt;</content><author><name></name></author><summary type="html">This is a survey of techniques and datasets for some vision and vision-language tasks. The survey was done on tasks which could be useful to a robot. A variety of tasks are explored: Extraction of relevant information from the surrounding scene This is done through the robot‚Äôs vision pipeline. Different extraction methods are applied on the robot‚Äôs video stream to extract different types of information from the scene. Object detection, event detection, human action detection and text detection and recognition are some of the key extraction methods applied. Text detection and recognition techniques are explored in more detail here. Retrieving extracted information based on natural language inputs provided by humans is also very important for a robot. Some techniques to retrieve objects in the scene based on their description are also explored. Navigation Navigation is a fundamental task for a robot. Recent techniques have explored navigation as a vision-language task in which the robot must navigate based on natural language commands and its video stream. Gaining visual knowledge In order to perform generic tasks such as zero-shot navigation in a new environment or visual-question answering based on the current scene, a robot must have large scale visual knowledge. A robot can either query an external knowledge base or accumulate knowledge in the parameters of a deep learning model. Recent work has explored the latter since end-to-end models with large scale visual knowledge perform better at visual tasks. Some papers proposing frameworks for large scale visual knowledge acquisition are explored. Miscellaneous Other interesting papers on datasets, custom architectures for video tasks, pretraining and finding specific moments in video are also surveyed.</summary></entry><entry><title type="html">Conversational Agents: A survey of techniques, frameworks and datasets</title><link href="http://localhost:4000/2022/11/01/ca-survey.html" rel="alternate" type="text/html" title="Conversational Agents: A survey of techniques, frameworks and datasets" /><published>2022-11-01T00:00:00-07:00</published><updated>2022-11-01T00:00:00-07:00</updated><id>http://localhost:4000/2022/11/01/ca-survey</id><content type="html" xml:base="http://localhost:4000/2022/11/01/ca-survey.html">&lt;p&gt;This article is a survey of conversational agents that I did when I was working to build one myself.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;architecture&quot;&gt;Architecture&lt;/h1&gt;
&lt;p&gt;The general architecture of a conversational agent is given in &lt;em&gt;Figure 1&lt;/em&gt;. Traditional techniques follow this architectural division closely and implement specialized modules for each component of the agent. However it is becoming increasingly common to combine one or more components in favour of more end-to-end conversational agents. This is especially true when using large deep learning models as they are able to implement more than one component, often with better results due to information sharing between the tasks of individual components.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ca/architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h4 align=&quot;center&quot;&gt;
&lt;i&gt;Figure 1: Architecture of a Conversational Agent&lt;/i&gt;
&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;state-of-the-art-techniques&quot;&gt;State-of-the-art techniques&lt;/h1&gt;
&lt;p&gt;Here‚Äôs a survey of state-of-the-art techniques for building entire conversational agents or one or more individual modules shown in &lt;em&gt;Figure 1&lt;/em&gt;. All papers are linked in the table and also at the end. A few other interesting papers, papers on human-in-the-loop techniques and studies on building conversational agents are also linked at the end.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Literature-Search.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Literature-Search.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4&gt;State-of-the-art techniques for each module&lt;/h4&gt;
&lt;p&gt;Here is a grouping of papers based on which architectural component their techniques apply to. The key points of each paper are summarized.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Papers-By-CA-Module.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Papers-By-CA-Module.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;frameworks&quot;&gt;Frameworks&lt;/h1&gt;
&lt;p&gt;Frameworks are an important starting point for the development of a conversational agent as they can provide a standard data format, pretrained models, dialog system pipelines, etc. I ultimately chose Rasa to implement my conversational agent but there are many frameworks out there suited for a wide variety of tasks. Here‚Äôs a survey of the most popular platforms and frameworks available.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Platforms-And-Tools.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Platforms-And-Tools.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;datasets&quot;&gt;Datasets&lt;/h1&gt;
&lt;p&gt;Datasets are necessary for training custom models to use for language understanding/dialog management or for training end-to-end conversational agents. Good quality datasets are very important for dialog applications since the agent needs to produce good quality text to hold a satisfying conversation. Here is a survey of the most popular datasets available for training conversational agents.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Datasets.pdf&quot;&gt;&lt;center&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/center&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Datasets.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;additional-notes-on-select-papers&quot;&gt;Additional notes on select papers&lt;/h1&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Paper-Notes.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Paper-Notes.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;&lt;i&gt;Speech and Language Processing&lt;/i&gt; by &lt;i&gt;Dan Jurafsky&lt;/i&gt; and &lt;i&gt;James H. Martin&lt;/i&gt;&lt;br /&gt;
Chapter 15: &lt;i&gt;Chatbots &amp;amp; Dialogue Systems&lt;/i&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;https://web.stanford.edu/~jurafsky/slp3/15.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Conversational Agents: Theory and Applications&lt;/i&gt; by &lt;i&gt;Mattias Wahde&lt;/i&gt; and &lt;i&gt;Marco Virgolin&lt;/i&gt;
&lt;a href=&quot;&quot;&gt;https://browse.arxiv.org/pdf/2202.03164.pdf&lt;/a&gt;&lt;br /&gt;
Here‚Äôs an extremely short summary of the key points they present.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;a href=&quot;/assets/ca/Conversational-Agents-Theory-and-Applications-Summary.pdf&quot;&gt;&lt;i&gt;Download the document or open it in a new tab&lt;/i&gt;&lt;/a&gt;
    &lt;object data=&quot;/assets/ca/Conversational-Agents-Theory-and-Applications-Summary.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;1000&quot;&gt;&lt;/object&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">This article is a survey of conversational agents that I did when I was working to build one myself. Architecture The general architecture of a conversational agent is given in Figure 1. Traditional techniques follow this architectural division closely and implement specialized modules for each component of the agent. However it is becoming increasingly common to combine one or more components in favour of more end-to-end conversational agents. This is especially true when using large deep learning models as they are able to implement more than one component, often with better results due to information sharing between the tasks of individual components.</summary></entry><entry><title type="html">MODALS: Data augmentation that works for everyone</title><link href="http://localhost:4000/2022/01/14/modals.html" rel="alternate" type="text/html" title="MODALS: Data augmentation that works for everyone" /><published>2022-01-14T00:00:00-08:00</published><updated>2022-01-14T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/14/modals</id><content type="html" xml:base="http://localhost:4000/2022/01/14/modals.html">&lt;p&gt;&lt;br /&gt;
In this article we discuss the paper ‚ÄúMODALS: Modality-agnostic Automated Data Augmentation in the Latent Space‚Äù[1]
&lt;br /&gt;
&lt;br /&gt;
Data augmentation is important for all types of deep learning tasks: unsupervised, supervised and self-supervised. Data augmentation has almost become a prerequisite for deep learning. In fact many self-supervised learning tasks [2] use data augmentation as a core component of the learning algorithm. It is also heavily used in image models and has consistently produced good results. By perturbing the data during training we are able to train more robust models, especially when there is limited data. Data augmentation is also very useful in incorporating known priors or domain knowledge into the model. For example the random flip transform applied to image data implicitly encodes the prior knowledge we have that the data is invariant to changes in orientation.
&lt;br /&gt;
&lt;br /&gt;
The usefulness of data augmentation has led to the development of specific techniques of augmentation unique to each modality of data. The techniques developed for one modality usually suit the type of data in that particular modality. For image data some commonly used augmentation techniques are rotation, cropping, applying affine transforms, random flips, contrast and color augmentations and adding Gaussian blur. More recent techniques like CutMix[3] and MixUp[4] apply data augmentation on both the image and label space. Many robust data augmentation techniques for image data already exist and are used widely. However modalities like tables and graph data don‚Äôt have as many robust augmentation techniques. Since the techniques developed for images were made taking into consideration the nature of image data, they usually cannot be applied to other modalities. If a generalized, modality-agnostic framework for augmentation could be developed then standard, robust augmentation techniques can be applied across many modalities. This is exactly what the authors of the paper propose.
&lt;br /&gt;
&lt;br /&gt;
In order to be independent of modality, such a framework would have to convert data from different modalities to a common representation on which label-preserving transforms can be applied. The authors point out that there exist techniques of data augmentation that use autoencoders to learn a latent space representation [5]. This is then used to generate augmentation data for the downstream task. In the framework proposed by the paper the learning of the latent space is integrated with the downstream task. A model is trained for the downstream task and an intermediate layer encodes the representations of the input data in the latent space. The framework uses interpolation and extrapolation of existing data points to produce augmented data. This requires that the latent space learned must be smooth (the representations cannot vary abruptly for a minute change in input features) for transformations to be label preserving and produce valid data. The smoothness ensures that the augmented data is inside the data manifold (ensuring it is valid data) and has the same label as the points used to generate it. To ensure this property the authors add two additional losses to the encoder used to learn the latent space. These are the adversarial loss and the triplet loss.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Adversarial Loss&lt;/h3&gt;
&lt;p&gt;To achieve a smooth latent space data manifold we use a discriminator network and add an adversarial loss to the model. The discriminator, given a data point, tries to predict if it is from the latent space or is a randomly sampled point from a Gaussian distribution. To keep the adversarial loss low (to ‚Äúfool‚Äù the discriminator network) the model must also learn a smooth data representation so as to be indistinguishable from a smooth Gaussian distribution.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Triplet Loss&lt;/h3&gt;
&lt;p&gt;The aim is to have the representations of all the data points of a class to be close to each other and far away from data points from other classes. To learn the latent space in this way we add the triplet loss to the model. The Triplet loss pulls together samples from the same class and pushes away samples from other classes.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;br /&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/PaperFig2.png&quot; alt=&quot;Figure 2 from the paper&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: The MODALS framework (figure from paper)
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Transformations in the latent space&lt;/h2&gt;
&lt;p&gt;Once the representations of points have been found, the authors propose the following transforms in the latent space for data augmentation:
&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Hard example interpolation&lt;/h3&gt;
&lt;p&gt;For each sample in the training set, the \(q\) nearest hard examples of that class are found. Then the data point is interpolated towards the nearest hard example to get a new augmented data point.
&lt;br /&gt;
In the figure below, the data point being considered is represented by \(\vec{\,a}\). \(q1\) through \(q5\) are five hard examples of the same class as \(\vec{\,a}\) and \(q3\) (represented by \(\vec{\,b}\)) is selected as it is the nearest to \(\vec{\,a}\) among the five. \(\vec{\,a}\) is interpolated towards \(\vec{\,b}\) with a scaling factor \(\lambda\) applied.
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/HardInterFig.png&quot; alt=&quot;Illustration of hard example interpolation&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: Illustration of hard example interpolation
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Hard example extrapolation&lt;/h3&gt;
&lt;p&gt;Similar to interpolation, each sample in the training set is considered and then shifted away from the class center to get the augmented data point.
&lt;br /&gt;
In the figure, the data point being considered is represented by \(\vec{\,a}\). The class center of \(\vec{\,a}\) is represented by \(\vec{\,b}\). \(\vec{\,a}\) is extrapolated away from \(\vec{\,b}\) by a factor \(\lambda\).
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/HardExtraFig.png&quot; alt=&quot;Illustration of hard example extrapolation&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: Illustration of hard example extrapolation
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Gaussian Noise&lt;/h3&gt;
&lt;p&gt;Gaussian noise is added to data points to get augmented data. The mean of the Gaussian noise is zero and the standard deviation is calculated on a per class basis.
&lt;br /&gt;
In the figure below, \(\vec{\,a}\) is the data point being considered and \(\vec{\,b}\), \(\vec{\,c}\) and \(\vec{\,d}\) are three points to which a may be transformed after adding Gaussian noise. The Gaussian noise which is scaled by a factor \(\lambda\) and added is shown as a circle.
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/GaussianFig.png&quot; alt=&quot;Illustration of addition of Gaussian noise&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: Illustration of addition of Gaussian noise
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Difference Transform&lt;/h3&gt;
&lt;p&gt;The rationale behind the difference transform is that labels should be independent of differences in features within a class. The difference between two samples from the same class is added to a data point to get an augmented instance of data. Adding the difference in features to the point should not change its label.
&lt;br /&gt;
The figure shows \(\vec{\,a}\) as the data point being considered and \(\vec{\,b}\) and \(\vec{\,c}\) are two randomly sampled points from the same class as \(\vec{\,a}\). The difference of \(\vec{\,b}\) and \(\vec{\,c}\) is added to \(\vec{\,a}\) to get the augmented point.
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/DiffTransform.png&quot; alt=&quot;Illustration of the difference transform&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: Illustration of the difference transform
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Population Based Augmentation (PBA)&lt;/h2&gt;
&lt;p&gt;Instead of fixing the set of augmentations to be applied to latent representations at all stages of training, the authors use a technique called PBA[6] to find optimized sets of augmentations to be applied at different stages. A few terms that PBA uses are defined below.
&lt;br /&gt;
&lt;br /&gt;
An augmentation function is defined as a 3-tuple consisting of a transformation, the probability of its application and its magnitude. An example given in the paper is that (op: rotation, p = 0.4, lambda = 0.5) is an augmentation function that applies the rotation transformation with a probability of 0.4 and a magnitude of 0.5.
&lt;br /&gt;
A policy is defined as the set of all 3-tuples which are to be applied to the dataset. 0, 1 or 2 tuples are randomly sampled from the current policy and applied to each mini-batch.
&lt;br /&gt;
A policy schedule is the order of policies which will be applied at different stages of training.
&lt;br /&gt;
&lt;br /&gt;
PBA searches for a good policy schedule which will give us optimized augmentation hyperparameters to be applied at different stages of training. PBA exploits population based training which is a method in which multiple child models are trained simultaneously to learn an optimal policy schedule. At the beginning each child is assigned a randomly sampled policy from the set of all possible policies (corresponding to all possible values of probability and magnitude). The child models are initialized with random weights and are trained for a fixed number of steps following which they are evaluated using the validation set. The evaluation reveals the worst and best of the models being trained. The worst models copy the weights and policies learned by the best models. The parameters (probability and magnitude) of the copied policies are then perturbed slightly or resampled from the set of all possible values. This is done to avoid creating duplicate models. This sequence of steps is repeated until the child models are done training. In the end, the best child model is selected and its policy schedule is used.
&lt;br /&gt;
There is reason to believe that this approach for finding a good policy schedule will work. The authors of [6] state that ‚Äú‚Ä¶ though the space of schedules is large, most good schedules are necessarily smooth and hence easily discoverable through evolutionary search algorithms such as PBT.‚Äù
&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;http://localhost:4000/assets/modals/PBAFig.png&quot; alt=&quot;The sequence of steps of PBA&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Figure: The sequence of steps of PBA
&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;In their experiments the authors applied the MODALS framework on text, tabular, time-series and image data. In general the framework works well across modalities except in the imaging modality where they found that using PBA with image specific data augmentation techniques performed better than MODALS. While MODALS is a powerful technique for data augmentation, domain specific augmentation techniques could still be used in certain contexts like image data. Domain knowledge and insights about the data can be incorporated through handcrafted augmentations but not in MODALS. As the authors themselves note, one possible approach is to use a combination of both, using handcrafted augmentations to incorporate domain knowledge and insights into the augmentation.
&lt;br /&gt;
&lt;br /&gt;
The framework proposed in the paper builds around classification tasks where each data point has a single label. There is potential for a technique like MODALS to be extended to other tasks like regression and multi-label classification.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Cheung, Tsz-Him, and Dit-Yan Yeung. ‚ÄúMODALS: Modality-agnostic Automated Data Augmentation in the Latent Space.‚Äù International Conference on Learning Representations. 2020.
&lt;br /&gt;
[2] He, Kaiming, et al. ‚ÄúMomentum contrast for unsupervised visual representation learning.‚Äù Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
&lt;br /&gt;
[3] Yun, Sangdoo, et al. ‚ÄúCutmix: Regularization strategy to train strong classifiers with localizable features.‚Äù Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.
&lt;br /&gt;
[4] Liu, Zicheng, et al. ‚ÄúAutoMix: Unveiling the Power of Mixup.‚Äù arXiv preprint arXiv:2103.13027 (2021).
&lt;br /&gt;
[5] DeVries, Terrance, and Graham W. Taylor. ‚ÄúDataset augmentation in feature space.‚Äù arXiv preprint arXiv:1702.05538 (2017).
&lt;br /&gt;
[6] Ho, Daniel, et al. ‚ÄúPopulation based augmentation: Efficient learning of augmentation policy schedules.‚Äù International Conference on Machine Learning. PMLR, 2019.
&lt;br /&gt;
&lt;br /&gt;
Images created using: The Manim Community Developers. (2022). Manim ‚Äì Mathematical Animation Framework (Version v0.14.0) [Computer software]. https://www.manim.community/&lt;/p&gt;</content><author><name></name></author><category term="deep learning" /><category term="data augmentation" /><category term="latent space" /><category term="data modalities" /><category term="automated data augmentation" /><summary type="html">In this article we discuss the paper ‚ÄúMODALS: Modality-agnostic Automated Data Augmentation in the Latent Space‚Äù[1] Data augmentation is important for all types of deep learning tasks: unsupervised, supervised and self-supervised. Data augmentation has almost become a prerequisite for deep learning. In fact many self-supervised learning tasks [2] use data augmentation as a core component of the learning algorithm. It is also heavily used in image models and has consistently produced good results. By perturbing the data during training we are able to train more robust models, especially when there is limited data. Data augmentation is also very useful in incorporating known priors or domain knowledge into the model. For example the random flip transform applied to image data implicitly encodes the prior knowledge we have that the data is invariant to changes in orientation. The usefulness of data augmentation has led to the development of specific techniques of augmentation unique to each modality of data. The techniques developed for one modality usually suit the type of data in that particular modality. For image data some commonly used augmentation techniques are rotation, cropping, applying affine transforms, random flips, contrast and color augmentations and adding Gaussian blur. More recent techniques like CutMix[3] and MixUp[4] apply data augmentation on both the image and label space. Many robust data augmentation techniques for image data already exist and are used widely. However modalities like tables and graph data don‚Äôt have as many robust augmentation techniques. Since the techniques developed for images were made taking into consideration the nature of image data, they usually cannot be applied to other modalities. If a generalized, modality-agnostic framework for augmentation could be developed then standard, robust augmentation techniques can be applied across many modalities. This is exactly what the authors of the paper propose. In order to be independent of modality, such a framework would have to convert data from different modalities to a common representation on which label-preserving transforms can be applied. The authors point out that there exist techniques of data augmentation that use autoencoders to learn a latent space representation [5]. This is then used to generate augmentation data for the downstream task. In the framework proposed by the paper the learning of the latent space is integrated with the downstream task. A model is trained for the downstream task and an intermediate layer encodes the representations of the input data in the latent space. The framework uses interpolation and extrapolation of existing data points to produce augmented data. This requires that the latent space learned must be smooth (the representations cannot vary abruptly for a minute change in input features) for transformations to be label preserving and produce valid data. The smoothness ensures that the augmented data is inside the data manifold (ensuring it is valid data) and has the same label as the points used to generate it. To ensure this property the authors add two additional losses to the encoder used to learn the latent space. These are the adversarial loss and the triplet loss. Adversarial Loss To achieve a smooth latent space data manifold we use a discriminator network and add an adversarial loss to the model. The discriminator, given a data point, tries to predict if it is from the latent space or is a randomly sampled point from a Gaussian distribution. To keep the adversarial loss low (to ‚Äúfool‚Äù the discriminator network) the model must also learn a smooth data representation so as to be indistinguishable from a smooth Gaussian distribution. Triplet Loss The aim is to have the representations of all the data points of a class to be close to each other and far away from data points from other classes. To learn the latent space in this way we add the triplet loss to the model. The Triplet loss pulls together samples from the same class and pushes away samples from other classes. Figure: The MODALS framework (figure from paper) Transformations in the latent space Once the representations of points have been found, the authors propose the following transforms in the latent space for data augmentation: Hard example interpolation For each sample in the training set, the \(q\) nearest hard examples of that class are found. Then the data point is interpolated towards the nearest hard example to get a new augmented data point. In the figure below, the data point being considered is represented by \(\vec{\,a}\). \(q1\) through \(q5\) are five hard examples of the same class as \(\vec{\,a}\) and \(q3\) (represented by \(\vec{\,b}\)) is selected as it is the nearest to \(\vec{\,a}\) among the five. \(\vec{\,a}\) is interpolated towards \(\vec{\,b}\) with a scaling factor \(\lambda\) applied. Figure: Illustration of hard example interpolation Hard example extrapolation Similar to interpolation, each sample in the training set is considered and then shifted away from the class center to get the augmented data point. In the figure, the data point being considered is represented by \(\vec{\,a}\). The class center of \(\vec{\,a}\) is represented by \(\vec{\,b}\). \(\vec{\,a}\) is extrapolated away from \(\vec{\,b}\) by a factor \(\lambda\). Figure: Illustration of hard example extrapolation Gaussian Noise Gaussian noise is added to data points to get augmented data. The mean of the Gaussian noise is zero and the standard deviation is calculated on a per class basis. In the figure below, \(\vec{\,a}\) is the data point being considered and \(\vec{\,b}\), \(\vec{\,c}\) and \(\vec{\,d}\) are three points to which a may be transformed after adding Gaussian noise. The Gaussian noise which is scaled by a factor \(\lambda\) and added is shown as a circle. Figure: Illustration of addition of Gaussian noise Difference Transform The rationale behind the difference transform is that labels should be independent of differences in features within a class. The difference between two samples from the same class is added to a data point to get an augmented instance of data. Adding the difference in features to the point should not change its label. The figure shows \(\vec{\,a}\) as the data point being considered and \(\vec{\,b}\) and \(\vec{\,c}\) are two randomly sampled points from the same class as \(\vec{\,a}\). The difference of \(\vec{\,b}\) and \(\vec{\,c}\) is added to \(\vec{\,a}\) to get the augmented point. Figure: Illustration of the difference transform Population Based Augmentation (PBA) Instead of fixing the set of augmentations to be applied to latent representations at all stages of training, the authors use a technique called PBA[6] to find optimized sets of augmentations to be applied at different stages. A few terms that PBA uses are defined below. An augmentation function is defined as a 3-tuple consisting of a transformation, the probability of its application and its magnitude. An example given in the paper is that (op: rotation, p = 0.4, lambda = 0.5) is an augmentation function that applies the rotation transformation with a probability of 0.4 and a magnitude of 0.5. A policy is defined as the set of all 3-tuples which are to be applied to the dataset. 0, 1 or 2 tuples are randomly sampled from the current policy and applied to each mini-batch. A policy schedule is the order of policies which will be applied at different stages of training. PBA searches for a good policy schedule which will give us optimized augmentation hyperparameters to be applied at different stages of training. PBA exploits population based training which is a method in which multiple child models are trained simultaneously to learn an optimal policy schedule. At the beginning each child is assigned a randomly sampled policy from the set of all possible policies (corresponding to all possible values of probability and magnitude). The child models are initialized with random weights and are trained for a fixed number of steps following which they are evaluated using the validation set. The evaluation reveals the worst and best of the models being trained. The worst models copy the weights and policies learned by the best models. The parameters (probability and magnitude) of the copied policies are then perturbed slightly or resampled from the set of all possible values. This is done to avoid creating duplicate models. This sequence of steps is repeated until the child models are done training. In the end, the best child model is selected and its policy schedule is used. There is reason to believe that this approach for finding a good policy schedule will work. The authors of [6] state that ‚Äú‚Ä¶ though the space of schedules is large, most good schedules are necessarily smooth and hence easily discoverable through evolutionary search algorithms such as PBT.‚Äù Figure: The sequence of steps of PBA Closing Thoughts In their experiments the authors applied the MODALS framework on text, tabular, time-series and image data. In general the framework works well across modalities except in the imaging modality where they found that using PBA with image specific data augmentation techniques performed better than MODALS. While MODALS is a powerful technique for data augmentation, domain specific augmentation techniques could still be used in certain contexts like image data. Domain knowledge and insights about the data can be incorporated through handcrafted augmentations but not in MODALS. As the authors themselves note, one possible approach is to use a combination of both, using handcrafted augmentations to incorporate domain knowledge and insights into the augmentation. The framework proposed in the paper builds around classification tasks where each data point has a single label. There is potential for a technique like MODALS to be extended to other tasks like regression and multi-label classification. References [1] Cheung, Tsz-Him, and Dit-Yan Yeung. ‚ÄúMODALS: Modality-agnostic Automated Data Augmentation in the Latent Space.‚Äù International Conference on Learning Representations. 2020. [2] He, Kaiming, et al. ‚ÄúMomentum contrast for unsupervised visual representation learning.‚Äù Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. [3] Yun, Sangdoo, et al. ‚ÄúCutmix: Regularization strategy to train strong classifiers with localizable features.‚Äù Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019. [4] Liu, Zicheng, et al. ‚ÄúAutoMix: Unveiling the Power of Mixup.‚Äù arXiv preprint arXiv:2103.13027 (2021). [5] DeVries, Terrance, and Graham W. Taylor. ‚ÄúDataset augmentation in feature space.‚Äù arXiv preprint arXiv:1702.05538 (2017). [6] Ho, Daniel, et al. ‚ÄúPopulation based augmentation: Efficient learning of augmentation policy schedules.‚Äù International Conference on Machine Learning. PMLR, 2019. Images created using: The Manim Community Developers. (2022). Manim ‚Äì Mathematical Animation Framework (Version v0.14.0) [Computer software]. https://www.manim.community/</summary></entry></feed>